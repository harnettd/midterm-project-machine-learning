{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from cross_validation import custom_cross_validation, hyperparameter_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5643, 51)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname = '../data/processed/'\n",
    "df = pd.read_csv(dirname + 'housing_data_2_trimmed.csv')\n",
    "\n",
    "# Drop non-numeric features, except 'postal_code'\n",
    "df = df.drop(columns=['city', 'state', 'sold_date'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which models are performing better, it's time to perform cross validation and tune hyperparameters.\n",
    "- Do a google search for hyperparameter ranges for each type of model.\n",
    "\n",
    "GridSearch/RandomSearch are a great methods for checking off both of these tasks.\n",
    "- BUT we have a problem - if we calculated a numerical value to encode city (such as the mean of sale prices in that city) on the training data, we can't cross validate \n",
    "- The rows in each validation fold were part of the original calculation of the mean for that city - that means we're leaking information!\n",
    "- While sklearn's built in functions are extremely useful, sometimes it is necessary to do things ourselves\n",
    "\n",
    "You need to create two functions to replicate what Gridsearch does under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`custom_cross_validation()`**\n",
    "- Should take the training data, and divide it into multiple train/validation splits. \n",
    "- Look into `sklearn.model_selection.KFold` to accomplish this - the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) shows how to split a dataframe and loop through the indexes of your split data. \n",
    "- Within your function, you should compute the city means on the training folds just like you did in Notebook 1 - you may have to re-join the city column to do this - and then join these values to the validation fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`hyperparameter_search()`**\n",
    "- Should take the validation and training splits from your previous function, along with your dictionary of hyperparameter values\n",
    "- For each set of hyperparameter values, fit your chosen model on each set of training folds, and take the average of your chosen scoring metric. [itertools.product()](https://docs.python.org/3/library/itertools.html) will be helpful for looping through all combinations of hyperparameter values\n",
    "- Your function should output the hyperparameter values corresponding the highest average score across all folds. Alternatively, it could also output a model object fit on the full training dataset with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_folds = custom_cross_validation(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 500000, 'gamma': 10, 'score': 0.8058787487874819}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = hyperparameter_search(\n",
    "    train_validate_folds[0], \n",
    "    train_validate_folds[1],\n",
    "    param_grid={\n",
    "        'C': [250_000, 500_000],\n",
    "        'gamma': [10, 50]\n",
    "    }\n",
    ")\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select the best SVM model that we found and pickle the model to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVR(\n",
    "    kernel='rbf',\n",
    "    C=best_model['C'],\n",
    "    gamma=best_model['gamma'],\n",
    "    epsilon=1.0\n",
    ")\n",
    "\n",
    "dirname = '../models/'\n",
    "basename = 'best_svm.pkl'\n",
    "with open(dirname + basename, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've identified which model works the best, implement a prediction pipeline to make sure that you haven't leaked any data, and that the model could be easily deployed if desired.\n",
    "- Your pipeline should load the data, process it, load your saved tuned model, and output a set of predictions\n",
    "- Assume that the new data is in the same JSON format as your original data - you can use your original data to check that the pipeline works correctly\n",
    "- Beware that a pipeline can only handle functions with fit and transform methods.\n",
    "- Classes can be used to get around this, but now sklearn has a wrapper for user defined functions.\n",
    "- You can develop your functions or classes in the notebook here, but once they are working, you should import them from `functions_variables.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines come from sklearn.  When a pipeline is pickled, all of the information in the pipeline is stored with it.  For example, if we were deploying a model, and we had fit a scaler on the training data, we would want the same, already fitted scaling object to transform the new data with.  This is all stored when the pipeline is pickled.\n",
    "- save your final pipeline in your `models/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your pipeline here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
