{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '../data/processed/'\n",
    "X_train = pd.read_csv(dirname + 'X_train_trimmed.csv', sep=',')\n",
    "X_test = pd.read_csv(dirname + 'X_test_trimmed.csv', sep=',')\n",
    "y_train = pd.read_csv(dirname + 'y_train_trimmed.csv', sep=',')\n",
    "y_test = pd.read_csv(dirname + 'y_test_trimmed.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Instructions (Delete Later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather evaluation metrics and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRETCH**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer,\\\n",
    "    PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "from utils import run_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a scaler to the feature data. Actually, some features should be scaled whereas others are Boolean and don't need to be scaled. We tried a few different scalers. `MinMaxScaler()` and `RobustScaler()` yielded similar results whereas the result from `PowerTransformer()` were a little worse than the other two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_reduced_amount', 'year_built', 'lot_sqft', 'sqft', 'baths', 'garage', 'stories', 'beds', 'median_by_pc']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of columns to be scaled.\n",
    "columns = X_train.columns.to_list()\n",
    "central_air_idx = columns.index('central_air')\n",
    "features_to_scale = columns[:central_air_idx]\n",
    "features_to_scale.append('median_by_pc')\n",
    "print(features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fts = X_train[features_to_scale]\n",
    "X_train_other = X_train.drop(columns=features_to_scale)\n",
    "\n",
    "X_test_fts = X_test[features_to_scale]\n",
    "X_test_other = X_test.drop(columns=features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a scaler.\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = RobustScaler()\n",
    "# scaler = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns that need scaling. Then, recombine with the other columns. \n",
    "X_train_sc = np.hstack([\n",
    "    scaler.fit_transform(X_train_fts), \n",
    "    X_train_other.to_numpy()\n",
    "])\n",
    "\n",
    "X_test_sc = np.hstack([\n",
    "    scaler.transform(X_test_fts), \n",
    "    X_test_other.to_numpy()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the k best features. To begin with, there are 50 features. However, many have low correlations with the target and can safely be dropped, leading to simpler models. Some trial and error indicates that k=8 is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(f_classif, k=8)\n",
    "\n",
    "X_train_sc_skb = skb.fit_transform(\n",
    "    X_train_sc, \n",
    "    np.ravel(y_train.to_numpy())\n",
    ")\n",
    "\n",
    "X_test_sc_skb = skb.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate polynomial features. Some trial and error seems to indicate that there is no reason to include polynomial features here; hence, we've set `degree=1` in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=1)\n",
    "\n",
    "X_train_sc_skb_poly = poly.fit_transform(X_train_sc_skb)\n",
    "X_test_sc_skb_poly = poly.transform(X_test_sc_skb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train: 89251.61664716937\n",
      "RMSE test: 86062.98812899395\n",
      "MAE train: 61213.46283121494\n",
      "MAE test: 61046.30914102518\n",
      "R**2 train: 0.765630007488411\n",
      "R**2 test: 0.7843355595723416\n",
      "Adj R**2 train: 0.765130403051508\n",
      "Adj R**2 test: 0.7829501349015001\n"
     ]
    }
   ],
   "source": [
    "model = run_regression(\n",
    "    [X_train_sc_skb_poly, y_train],\n",
    "    [X_test_sc_skb_poly, y_test],\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over a few values of the hyperparameter alpha. Doing so doesn't appear to have much effect on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.01\n",
      "RMSE train: 89251.61666170432\n",
      "RMSE test: 86063.11627149125\n",
      "MAE train: 61213.408181275925\n",
      "MAE test: 61046.306809453046\n",
      "R**2 train: 0.7656300074120751\n",
      "R**2 test: 0.7843349173496823\n",
      "Adj R**2 train: 0.7651304029750094\n",
      "Adj R**2 test: 0.7829494885532134\n",
      "\n",
      "alpha = 0.1\n",
      "RMSE train: 89251.61809835208\n",
      "RMSE test: 86064.27013338474\n",
      "MAE train: 61212.916932103646\n",
      "MAE test: 61046.2861523519\n",
      "R**2 train: 0.765629999866954\n",
      "R**2 test: 0.7843291343987603\n",
      "Adj R**2 train: 0.7651303954138045\n",
      "Adj R**2 test: 0.7829436684527138\n",
      "\n",
      "alpha = 1\n",
      "RMSE train: 89251.75948763241\n",
      "RMSE test: 86075.8643164402\n",
      "MAE train: 61208.06326392328\n",
      "MAE test: 61046.11153113789\n",
      "R**2 train: 0.7656292573049119\n",
      "R**2 test: 0.7842710221084586\n",
      "Adj R**2 train: 0.7651296512688495\n",
      "Adj R**2 test: 0.7828851828500547\n",
      "\n",
      "alpha = 10\n",
      "RMSE train: 89263.91730220409\n",
      "RMSE test: 86195.83894571947\n",
      "MAE train: 61165.628526310684\n",
      "MAE test: 61053.90533074585\n",
      "R**2 train: 0.7655654013065272\n",
      "R**2 test: 0.7836692264288947\n",
      "Adj R**2 train: 0.7650656591491986\n",
      "Adj R**2 test: 0.7822795212453544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.1, 1, 10]\n",
    "for alpha in alphas:\n",
    "    print(f'alpha = {alpha}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, y_train],\n",
    "        [X_test_sc_skb_poly, y_test],\n",
    "        Ridge(alpha=alpha)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we loop through several values of alpha to asses the parameter's impact on the analysis. As with the Ridge regression analysis above, changing alpha does not have much effect on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.1\n",
      "RMSE train: 89251.61665290635\n",
      "RMSE test: 86063.04462270497\n",
      "MAE train: 61213.41380193851\n",
      "MAE test: 61046.267419976146\n",
      "R**2 train: 0.7656300074582811\n",
      "R**2 test: 0.78433527643812\n",
      "Adj R**2 train: 0.765130403021314\n",
      "Adj R**2 test: 0.7829498499484291\n",
      "\n",
      "alpha = 1\n",
      "RMSE train: 89251.61721756474\n",
      "RMSE test: 86063.55819614459\n",
      "MAE train: 61212.943755002125\n",
      "MAE test: 61045.868023732415\n",
      "R**2 train: 0.7656300044927554\n",
      "R**2 test: 0.784332702510952\n",
      "Adj R**2 train: 0.7651304000494666\n",
      "Adj R**2 test: 0.7829472594863971\n",
      "\n",
      "alpha = 10\n",
      "RMSE train: 89251.67369078356\n",
      "RMSE test: 86068.8199203835\n",
      "MAE train: 61208.077184139045\n",
      "MAE test: 61041.774336419076\n",
      "R**2 train: 0.765629707901329\n",
      "R**2 test: 0.7843063309186202\n",
      "Adj R**2 train: 0.7651301028257989\n",
      "Adj R**2 test: 0.7829207184834079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 1, 10]\n",
    "for alpha in alphas:\n",
    "    print(f'alpha = {alpha}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, y_train],\n",
    "        [X_test_sc_skb_poly, y_test],\n",
    "        Lasso(alpha=alpha)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For our SVM investigations, we continue to use the dataset from the Linear Regression section above. Recall that this dataset has been scaled. Also, from this dataset, we work with the k=8 best features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the category of SVMs, we first look at a linear kernel. Through trial and error, we find a good value of the hyperparameter C of ~350."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 300\n",
      "RMSE train: 92209.48718993028\n",
      "RMSE test: 90426.64161958164\n",
      "MAE train: 59607.367412719854\n",
      "MAE test: 59455.536016781094\n",
      "R**2 train: 0.7498381748012491\n",
      "R**2 test: 0.7619114529348443\n",
      "Adj R**2 train: 0.7493049070544967\n",
      "Adj R**2 test: 0.7603819761871025\n",
      "\n",
      "C = 350\n",
      "RMSE train: 92245.25422686318\n",
      "RMSE test: 90323.22653533611\n",
      "MAE train: 59175.919932360746\n",
      "MAE test: 58930.07049544549\n",
      "R**2 train: 0.7496440671700073\n",
      "R**2 test: 0.7624557143925299\n",
      "Adj R**2 train: 0.7491103856457368\n",
      "Adj R**2 test: 0.7609297339710687\n",
      "\n",
      "C = 400\n",
      "RMSE train: 92316.32861722662\n",
      "RMSE test: 90356.26632702498\n",
      "MAE train: 59035.72708948502\n",
      "MAE test: 58741.02061375538\n",
      "R**2 train: 0.7492581231885771\n",
      "R**2 test: 0.7622818975467414\n",
      "Adj R**2 train: 0.7487236189509402\n",
      "Adj R**2 test: 0.7607548005288404\n",
      "\n",
      "C = 450\n",
      "RMSE train: 92511.56197034537\n",
      "RMSE test: 90515.91693536108\n",
      "MAE train: 58900.084984945\n",
      "MAE test: 58529.13531836098\n",
      "R**2 train: 0.7481964487927597\n",
      "R**2 test: 0.761441106535968\n",
      "Adj R**2 train: 0.7476596813932179\n",
      "Adj R**2 test: 0.759908608291017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C_values = list(np.arange(300, 500, 50))\n",
    "for C in C_values:\n",
    "    print(f'C = {C}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, np.ravel(y_train)],\n",
    "        [X_test_sc_skb_poly, np.ravel(y_test)],\n",
    "        svm.SVR(kernel='linear', C=C)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we consider the 'rbf' kernel. Through trial and error, we found the good values of the hyperparameters C=100,000 and gamma=2. The hyperparameter epsilon appears to have little effect on the analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 0.01\n",
      "RMSE train: 75891.06462011661\n",
      "RMSE test: 79054.05813524779\n",
      "MAE train: 43384.79660196193\n",
      "MAE test: 50021.992379172465\n",
      "R**2 train: 0.8305462774233964\n",
      "R**2 test: 0.8180324118839486\n",
      "Adj R**2 train: 0.8301850544240621\n",
      "Adj R**2 test: 0.8168634552151088\n",
      "\n",
      "eps = 0.1\n",
      "RMSE train: 75891.0727791279\n",
      "RMSE test: 79054.07108678637\n",
      "MAE train: 43384.80866476328\n",
      "MAE test: 50021.99440336805\n",
      "R**2 train: 0.8305462409876205\n",
      "R**2 test: 0.8180323522599272\n",
      "Adj R**2 train: 0.8301850179106164\n",
      "Adj R**2 test: 0.8168633952080637\n",
      "\n",
      "eps = 1\n",
      "RMSE train: 75891.15434895335\n",
      "RMSE test: 79054.20053226083\n",
      "MAE train: 43384.96000516488\n",
      "MAE test: 50022.02164180411\n",
      "R**2 train: 0.8305458767202445\n",
      "R**2 test: 0.818031756341021\n",
      "Adj R**2 train: 0.8301846528667348\n",
      "Adj R**2 test: 0.8168627954609847\n",
      "\n",
      "eps = 10\n",
      "RMSE train: 75892.06109790191\n",
      "RMSE test: 79055.80534323858\n",
      "MAE train: 43386.48389773113\n",
      "MAE test: 50022.163204172015\n",
      "R**2 train: 0.8305418274139096\n",
      "R**2 test: 0.8180243683060494\n",
      "Adj R**2 train: 0.8301805949285295\n",
      "Adj R**2 test: 0.816855359965403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps_values = [0.01, 0.1, 1, 10]\n",
    "for eps in eps_values:\n",
    "    print(f'eps = {eps}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, np.ravel(y_train)],\n",
    "        [X_test_sc_skb_poly, np.ravel(y_test)],\n",
    "        svm.SVR(kernel='rbf', C=100_000, gamma=2, epsilon=eps)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
