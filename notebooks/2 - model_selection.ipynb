{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '../data/processed/'\n",
    "X_train = pd.read_csv(dirname + 'X_train_trimmed.csv', sep=',')\n",
    "X_test = pd.read_csv(dirname + 'X_test_trimmed.csv', sep=',')\n",
    "y_train = pd.read_csv(dirname + 'y_train_trimmed.csv', sep=',')\n",
    "y_test = pd.read_csv(dirname + 'y_test_trimmed.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions (Delete Later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather evaluation metrics and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRETCH**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "from utils import print_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a scaler to the feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_train_sc_skb = skb.fit_transform(\n",
    "    X_train_sc, \n",
    "    np.ravel(y_train.to_numpy())\n",
    ")\n",
    "\n",
    "X_test_sc_skb = skb.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(\n",
    "    train: [np.array, np.array],\n",
    "    test: [np.array, np.array],\n",
    "    regressor\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a fitted regressor.\n",
    "\n",
    "    Fit the regressor and print out a variety of test score.\n",
    "\n",
    "    :param train: [X_train, y_train]\n",
    "    :param test: [X_test, y_train]\n",
    "    :param regressor: a regression model instance such as LinearRegression()\n",
    "\n",
    "    :return: A fitted regression model\n",
    "    \"\"\"\n",
    "    X_train, y_train = train\n",
    "    X_test, y_test = test\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = regressor.predict(X_train)\n",
    "    y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "    print_scores(\n",
    "        [X_train, y_train, y_train_pred], \n",
    "        [X_test, y_test, y_test_pred]\n",
    "    )\n",
    "\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train: 100679.25895530147\n",
      "RMSE test: 97592.40727636927\n",
      "MAE train: 65083.039461415436\n",
      "MAE test: 62743.96514556088\n",
      "R**2 train: 0.7017710194590668\n",
      "R**2 test: 0.7226821894989526\n",
      "Adj R**2 train: 0.7014181692691226\n",
      "Adj R**2 test: 0.721695293376173\n"
     ]
    }
   ],
   "source": [
    "model = run_regression(\n",
    "    [X_train_sc_skb, y_train],\n",
    "    [X_test_sc_skb, y_test],\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features: 5\n",
      "Coefficients: [[ -85321.3953897  -102526.8771943    16141.3107541     6723.54063453\n",
      "   792832.60401293]]\n"
     ]
    }
   ],
   "source": [
    "print(f'No. features: {model.n_features_in_}')\n",
    "print(f'Coefficients: {model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train: 101124.86896744039\n",
      "RMSE test: 98601.55836232286\n",
      "MAE train: 67542.41903160974\n",
      "MAE test: 65789.79370211065\n",
      "R**2 train: 0.6991252328804254\n",
      "R**2 test: 0.71691734577513\n",
      "Adj R**2 train: 0.6987692523230193\n",
      "Adj R**2 test: 0.7159099341942585\n"
     ]
    }
   ],
   "source": [
    "model = run_regression(\n",
    "    [X_train_sc_skb, y_train],\n",
    "    [X_test_sc_skb, y_test],\n",
    "    Ridge(alpha=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train: 100679.25975472081\n",
      "RMSE test: 97592.99014440672\n",
      "MAE train: 65083.291426962904\n",
      "MAE test: 62744.27165503323\n",
      "R**2 train: 0.7017710147230364\n",
      "R**2 test: 0.7226788769426755\n",
      "Adj R**2 train: 0.7014181645274886\n",
      "Adj R**2 test: 0.7216919690314394\n"
     ]
    }
   ],
   "source": [
    "model = run_regression(\n",
    "    [X_train_sc_skb, y_train],\n",
    "    [X_test_sc_skb, y_test],\n",
    "    Lasso(alpha=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features: 5\n",
      "Coefficients: [ -85301.85394224 -102485.06478651   16093.41904181    6702.66215753\n",
      "  792802.9298412 ]\n"
     ]
    }
   ],
   "source": [
    "print(f'No. features: {model.n_features_in_}')\n",
    "print(f'Coefficients: {model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
