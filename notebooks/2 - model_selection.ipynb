{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#specific to R-SQUARE metric\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#specific to Linear Regression \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer,\\\n",
    "    PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "from utils import run_regression\n",
    "\n",
    "#specific to support vector machine\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '../data/processed/'\n",
    "X_train = pd.read_csv(dirname + 'X_train_trimmed.csv', sep=',')\n",
    "X_test = pd.read_csv(dirname + 'X_test_trimmed.csv', sep=',')\n",
    "y_train = pd.read_csv(dirname + 'y_train_trimmed.csv', sep=',')\n",
    "y_test = pd.read_csv(dirname + 'y_test_trimmed.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4232, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price_reduced_amount', 'year_built', 'lot_sqft', 'sqft', 'baths',\n",
       "       'garage', 'stories', 'beds', 'central_air', 'central_heat', 'fireplace',\n",
       "       'rental_property', 'energy_efficient', 'community_security_features',\n",
       "       'carport', 'dishwasher', 'washer_dryer', 'laundry_room', 'floor_plan',\n",
       "       'ensuite', 'shopping', 'hardwood_floors', 'high_ceiling',\n",
       "       'open_floor_plan', 'fenced_yard', 'new_roof', 'front_porch',\n",
       "       'groundscare', 'basement', 'corner_lot', 'farm', 'ranch', 'forced_air',\n",
       "       'dining_room', 'family_room', 'view', 'near_outdoors',\n",
       "       'near_rec_facilities', 'fancy_kitchen', 'type_condo', 'type_land',\n",
       "       'type_mobile', 'type_multi_family', 'type_other', 'type_single_family',\n",
       "       'type_townhome', 'season_autumn', 'season_summer', 'season_winter',\n",
       "       'median_by_pc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a scaler to the feature data. Actually, some features should be scaled whereas others are Boolean and don't need to be scaled. We tried a few different scalers. `MinMaxScaler()` and `RobustScaler()` yielded similar results whereas the result from `PowerTransformer()` were a little worse than the other two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_reduced_amount', 'year_built', 'lot_sqft', 'sqft', 'baths', 'garage', 'stories', 'beds', 'median_by_pc']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of columns to be scaled.\n",
    "columns = X_train.columns.to_list()\n",
    "central_air_idx = columns.index('central_air')\n",
    "features_to_scale = columns[:central_air_idx]\n",
    "features_to_scale.append('median_by_pc')\n",
    "print(features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fts = X_train[features_to_scale]\n",
    "X_train_other = X_train.drop(columns=features_to_scale)\n",
    "\n",
    "X_test_fts = X_test[features_to_scale]\n",
    "X_test_other = X_test.drop(columns=features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a scaler.\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = RobustScaler()\n",
    "# scaler = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns that need scaling. Then, recombine with the other columns. \n",
    "X_train_sc = np.hstack([\n",
    "    scaler.fit_transform(X_train_fts), \n",
    "    X_train_other.to_numpy()\n",
    "])\n",
    "\n",
    "X_test_sc = np.hstack([\n",
    "    scaler.transform(X_test_fts), \n",
    "    X_test_other.to_numpy()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the k best features. To begin with, there are 50 features. However, many have low correlations with the target and can safely be dropped, leading to simpler models. Some trial and error indicates that k=8 is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(f_classif, k=8)\n",
    "\n",
    "X_train_sc_skb = skb.fit_transform(\n",
    "    X_train_sc, \n",
    "    np.ravel(y_train.to_numpy())\n",
    ")\n",
    "\n",
    "X_test_sc_skb = skb.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate polynomial features. Some trial and error seems to indicate that there is no reason to include polynomial features here; hence, we've set `degree=1` in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=1)\n",
    "\n",
    "X_train_sc_skb_poly = poly.fit_transform(X_train_sc_skb)\n",
    "X_test_sc_skb_poly = poly.transform(X_test_sc_skb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train: 89252.87062147324\n",
      "RMSE test: 86063.44290192694\n",
      "MAE train: 61218.44565217391\n",
      "MAE test: 61051.7434443657\n",
      "R**2 train: 0.7656234217026554\n",
      "R**2 test: 0.7843332803438049\n",
      "Adj R**2 train: 0.7651238032268912\n",
      "Adj R**2 test: 0.7829478410312383\n"
     ]
    }
   ],
   "source": [
    "model = run_regression(\n",
    "    [X_train_sc_skb_poly, y_train],\n",
    "    [X_test_sc_skb_poly, y_test],\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over a few values of the hyperparameter alpha. Doing so doesn't appear to have much effect on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.01\n",
      "RMSE train: 89251.61714016362\n",
      "RMSE test: 86063.53324387537\n",
      "MAE train: 61213.81236779648\n",
      "MAE test: 61047.03226436391\n",
      "R**2 train: 0.7656300048992578\n",
      "R**2 test: 0.7843328275670803\n",
      "Adj R**2 train: 0.7651304004568356\n",
      "Adj R**2 test: 0.7829473853458838\n",
      "\n",
      "alpha = 0.1\n",
      "RMSE train: 89251.66587880427\n",
      "RMSE test: 86068.47945877376\n",
      "MAE train: 61216.95374131726\n",
      "MAE test: 61053.53005693284\n",
      "R**2 train: 0.7656297489290367\n",
      "R**2 test: 0.7843080373500226\n",
      "Adj R**2 train: 0.765130143940965\n",
      "Adj R**2 test: 0.7829224358768965\n",
      "\n",
      "alpha = 1\n",
      "RMSE train: 89256.47290695073\n",
      "RMSE test: 86121.85685060041\n",
      "MAE train: 61248.60254135914\n",
      "MAE test: 61119.70993924571\n",
      "R**2 train: 0.7656045022353851\n",
      "R**2 test: 0.784040421504394\n",
      "Adj R**2 train: 0.7651048434291602\n",
      "Adj R**2 test: 0.78265310087166\n",
      "\n",
      "alpha = 10\n",
      "RMSE train: 89677.8949676224\n",
      "RMSE test: 86991.59909339724\n",
      "MAE train: 61978.95173337249\n",
      "MAE test: 62135.71752462698\n",
      "R**2 train: 0.7633858928127034\n",
      "R**2 test: 0.7796564563685622\n",
      "Adj R**2 train: 0.7628815046164255\n",
      "Adj R**2 test: 0.7782409732188955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.1, 1, 10]\n",
    "for alpha in alphas:\n",
    "    print(f'alpha = {alpha}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, y_train],\n",
    "        [X_test_sc_skb_poly, y_test],\n",
    "        Ridge(alpha=alpha)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we loop through several values of alpha to asses the parameter's impact on the analysis. As with the Ridge regression analysis above, changing alpha does not have much effect on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.1\n",
      "RMSE train: 89251.61665544404\n",
      "RMSE test: 86063.04306307028\n",
      "MAE train: 61213.377124843384\n",
      "MAE test: 61046.217569771805\n",
      "R**2 train: 0.7656300074449535\n",
      "R**2 test: 0.7843352842546732\n",
      "Adj R**2 train: 0.7651304030079579\n",
      "Adj R**2 test: 0.7829498578151958\n",
      "\n",
      "alpha = 1\n",
      "RMSE train: 89251.61748062872\n",
      "RMSE test: 86063.54065307337\n",
      "MAE train: 61212.599184713836\n",
      "MAE test: 61045.38765912565\n",
      "R**2 train: 0.7656300031111714\n",
      "R**2 test: 0.784332790433563\n",
      "Adj R**2 train: 0.7651303986649375\n",
      "Adj R**2 test: 0.7829473479738214\n",
      "\n",
      "alpha = 10\n",
      "RMSE train: 89251.69975241416\n",
      "RMSE test: 86068.81592408966\n",
      "MAE train: 61204.25330173423\n",
      "MAE test: 61036.765567275266\n",
      "R**2 train: 0.7656295710283132\n",
      "R**2 test: 0.7843063509485279\n",
      "Adj R**2 train: 0.7651299656610121\n",
      "Adj R**2 test: 0.7829207386419873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 1, 10]\n",
    "for alpha in alphas:\n",
    "    print(f'alpha = {alpha}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, y_train],\n",
    "        [X_test_sc_skb_poly, y_test],\n",
    "        Lasso(alpha=alpha)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For our SVM investigations, we continue to use the dataset from the Linear Regression section above. Recall that this dataset has been scaled. Also, from this dataset, we work with the k=8 best features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the category of SVMs, we first look at a linear kernel. Through trial and error, we find a good value of the hyperparameter C of ~350."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 300\n",
      "RMSE train: 151912.97717673305\n",
      "RMSE test: 153606.37420887017\n",
      "MAE train: 117324.91520322875\n",
      "MAE test: 118827.16812995951\n",
      "R**2 train: 0.3210159482213766\n",
      "R**2 test: 0.3129882304605941\n",
      "Adj R**2 train: 0.3195685639328858\n",
      "Adj R**2 test: 0.30857487862201116\n",
      "\n",
      "C = 350\n",
      "RMSE train: 147228.81850514354\n",
      "RMSE test: 148902.34876912585\n",
      "MAE train: 113270.66337602049\n",
      "MAE test: 114808.95700469491\n",
      "R**2 train: 0.3622426429668929\n",
      "R**2 test: 0.35442188734133084\n",
      "Adj R**2 train: 0.36088314125839027\n",
      "Adj R**2 test: 0.3502747046047655\n",
      "\n",
      "C = 400\n",
      "RMSE train: 142505.90961252525\n",
      "RMSE test: 144114.63437577695\n",
      "MAE train: 109445.80963353819\n",
      "MAE test: 111101.88559941734\n",
      "R**2 train: 0.4025032153445478\n",
      "R**2 test: 0.39526950467266375\n",
      "Adj R**2 train: 0.40122953674153994\n",
      "Adj R**2 test: 0.3913847263300898\n",
      "\n",
      "C = 450\n",
      "RMSE train: 138311.66592826872\n",
      "RMSE test: 139810.5490729341\n",
      "MAE train: 105942.3587122938\n",
      "MAE test: 107600.71059068604\n",
      "R**2 train: 0.437156768267597\n",
      "R**2 test: 0.43085151540516786\n",
      "Adj R**2 train: 0.4359569603363814\n",
      "Adj R**2 test: 0.42719531528999766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C_values = list(np.arange(300, 500, 50))\n",
    "for C in C_values:\n",
    "    print(f'C = {C}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, np.ravel(y_train)],\n",
    "        [X_test_sc_skb_poly, np.ravel(y_test)],\n",
    "        svm.SVR(kernel='linear', C=C)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we consider the 'rbf' kernel. Through trial and error, we found the good values of the hyperparameters C=100,000 and gamma=2. The hyperparameter epsilon appears to have little effect on the analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 0.01\n",
      "RMSE train: 88485.36479919084\n",
      "RMSE test: 85420.65849326775\n",
      "MAE train: 53971.87252152956\n",
      "MAE test: 53702.887503314596\n",
      "R**2 train: 0.7696370057908679\n",
      "R**2 test: 0.7875427619957461\n",
      "Adj R**2 train: 0.7691459430367509\n",
      "Adj R**2 test: 0.7861779403383312\n",
      "\n",
      "eps = 0.1\n",
      "RMSE train: 88485.33318969741\n",
      "RMSE test: 85420.62691517737\n",
      "MAE train: 53971.87448307618\n",
      "MAE test: 53702.8897149648\n",
      "R**2 train: 0.7696371703752884\n",
      "R**2 test: 0.7875429190770129\n",
      "Adj R**2 train: 0.7691461079720145\n",
      "Adj R**2 test: 0.7861780984286854\n",
      "\n",
      "eps = 1\n",
      "RMSE train: 88485.01705532643\n",
      "RMSE test: 85420.31121135446\n",
      "MAE train: 53971.90173553646\n",
      "MAE test: 53702.91418982472\n",
      "R**2 train: 0.7696388164215997\n",
      "R**2 test: 0.7875444895030593\n",
      "Adj R**2 train: 0.7691477575271881\n",
      "Adj R**2 test: 0.7861796789431218\n",
      "\n",
      "eps = 10\n",
      "RMSE train: 88482.15046823306\n",
      "RMSE test: 85416.85500086556\n",
      "MAE train: 53972.51832676385\n",
      "MAE test: 53702.75142060538\n",
      "R**2 train: 0.7696537418795175\n",
      "R**2 test: 0.7875616815759572\n",
      "Adj R**2 train: 0.7691627148015724\n",
      "Adj R**2 test: 0.7861969814576014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps_values = [0.01, 0.1, 1, 10]\n",
    "for eps in eps_values:\n",
    "    print(f'eps = {eps}')\n",
    "    model = run_regression(\n",
    "        [X_train_sc_skb_poly, np.ravel(y_train)],\n",
    "        [X_test_sc_skb_poly, np.ravel(y_test)],\n",
    "        svm.SVR(kernel='rbf', C=100_000, gamma=2, epsilon=eps)\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 19271.749725687634\n",
      "Test RMSE: 24557.998581820277\n",
      "Train R-squared: 0.9890292313107282\n",
      "Test R-squared: 0.9827582511531365\n"
     ]
    }
   ],
   "source": [
    "# Concatenating datasets to form one common dataset\n",
    "data_x = pd.concat([X_train, X_test], ignore_index=True)\n",
    "data_y = pd.concat([y_train, y_test], ignore_index=True)\n",
    "\n",
    "# Creating X and y from datasets\n",
    "def combine_first_row_x(series):\n",
    "    return series.combine_first(data_x.iloc[0])\n",
    "\n",
    "def combine_first_row_y(series):\n",
    "    return series.combine_first(data_y.iloc[0])\n",
    "\n",
    "X = data_x.apply(combine_first_row_x, axis=1)\n",
    "y = data_y.apply(combine_first_row_y, axis=1)\n",
    "\n",
    "# Splitting X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training and fitting\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Model training and evaluation\n",
    "train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "\n",
    "# Adding R-SQUARE calculation\n",
    "# Calculating r-squared score\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Evaluating model using r2\n",
    "print(\"Train R-squared:\", train_r2)\n",
    "print(\"Test R-squared:\", test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model assesses  performance based on two metrics: Root Mean Squared Error and R-square. These are two metrics that works well with our dataset. Firstly, RMSE is a good metric to consider since it is already sensitive to outliers. So the performance to similar dataset will at least reflect same degree of error. Combining this metric to r-square makes more sense because it shows the level of dependence between the dataset and the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zw/340m7hr15_9btprp9l72ph_80000gn/T/ipykernel_1260/3003391060.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 19465.721152458038\n",
      "Test RMSE: 28866.70519734506\n",
      "Train R-squared: 0.9888072768961536\n",
      "Test R-squared: 0.9761773643432353\n"
     ]
    }
   ],
   "source": [
    "#using the same dataset as above, we can train a different model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predicting the model results\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# evaluating the model results on RMSE and r2\n",
    "train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "\n",
    "#let us add R-SQUARE \n",
    "#calculating r-squared score\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "#evaluating random forest using r2\n",
    "print(\"Train R-squared:\", train_r2)\n",
    "print(\"Test R-squared:\", test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as the XGBoost model, Random Forest measures the same metrics but on a different model. The idea is to keep the consistency while changing the model to see what score is better for our data. Another idea is also to capitalize on variables relationship will readucing the RMSE as much as possible. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
